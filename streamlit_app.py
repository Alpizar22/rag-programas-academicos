import streamlit as st
import pandas as pd
import openai
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import tiktoken
import os

# === Cargar datos ===
brochures = pd.read_csv("brochures_extraidos.csv")
perfil_df = pd.read_excel("perfil_completo_sibila_resumen.xlsx", sheet_name="Descripci√≥n del perfil")

# === Inicializar modelo de embeddings ===
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# === Embeddings del brochure ===
brochures = brochures.dropna(subset=["Texto Brochure"])
brochures["embedding"] = brochures["Texto Brochure"].apply(lambda x: embedder.encode(x, convert_to_tensor=False))

# === Sin√≥nimos para expansi√≥n de preguntas ===
sinonimos = {
    "materias": ["asignaturas", "temario", "contenidos", "plan de estudios"],
    "duraci√≥n": ["cu√°nto dura", "tiempo de estudio", "a√±os del programa"],
    "costo": ["precio", "valor", "cu√°nto cuesta", "tarifa"],
    "perfil": ["tipo de estudiante", "caracter√≠sticas del aspirante", "perfil del alumno"],
}

def expandir_pregunta(pregunta):
    preguntas = [pregunta]
    for palabra, variantes in sinonimos.items():
        if palabra in pregunta.lower():
            preguntas += [pregunta.lower().replace(palabra, alt) for alt in variantes]
    return preguntas

# === Funci√≥n para obtener tokens ===
def contar_tokens(texto, modelo="gpt-4"):
    encoding = tiktoken.encoding_for_model(modelo)
    return len(encoding.encode(texto))

# === Configuraci√≥n OpenAI ===
openai.api_key = st.secrets["openai_api_key"]

# === Historial de conversaci√≥n ===
if "historial" not in st.session_state:
    st.session_state.historial = []

# === Interfaz ===
st.title("üîç Asistente de Programas Acad√©micos")
pregunta = st.text_input("¬øQu√© quieres saber sobre un programa acad√©mico?")

if pregunta:
    st.session_state.historial.append({"role": "user", "content": pregunta})
    
    pregunta_embedding = embedder.encode(pregunta, convert_to_tensor=False)
    brochures["score"] = brochures["embedding"].apply(lambda x: cosine_similarity([x], [pregunta_embedding])[0][0])
    top = brochures.sort_values("score", ascending=False).iloc[0]

    # Limitar el contexto a 500 tokens
    contexto = top["Texto Brochure"]
    while contar_tokens(contexto) > 500:
        contexto = contexto[:len(contexto) - 100]

    # Construir prompt con historial
    mensajes = st.session_state.historial.copy()
    mensajes.insert(0, {"role": "system", "content": "Eres un asistente experto en programas acad√©micos. Usa el contexto proporcionado para responder con claridad y precisi√≥n. Si no hay informaci√≥n relevante, responde solo si el programa es 'Anal√≠tica de Negocios', usando el perfil t√≠pico como sugerencia secundaria."})
    mensajes.append({"role": "user", "content": f"CONTEXTO:\n{contexto}\n\nPregunta: {pregunta}\nResponde en espa√±ol, con m√°ximo 600 tokens."})

    respuesta = openai.ChatCompletion.create(
        model="gpt-4",
        messages=mensajes,
        max_tokens=600
    )["choices"][0]["message"]["content"]

    # Si la respuesta es irrelevante y es Anal√≠tica de Negocios, usar perfil interpretativo como respaldo
    if ("no tengo" in respuesta.lower() or len(respuesta.strip()) < 50) and "anal√≠tica de negocios" in pregunta.lower():
        respuesta = perfil_df.iloc[0, 0]

    st.session_state.historial.append({"role": "assistant", "content": respuesta})

    st.markdown("### üìò Respuesta:")
    st.write(respuesta)

    st.markdown("---")
    st.markdown(f"üîé Contexto usado (truncado):\n\n{contexto[:1000]}...")

